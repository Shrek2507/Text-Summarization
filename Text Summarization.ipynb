{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Performing Text summarization using Extractive Summarization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wordninja\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk.corpus import words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import networkx as nx\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsParsing(object):\n",
    "    def __init__(self):\n",
    "        self.regex_pattern = r'[^A-Za-z0-9.]'\n",
    "        self.stopwords_list = stopwords.words('english')\n",
    "        self.words_list = list(words.words())\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.file_path = \"./Articles/\"\n",
    "    \n",
    "    # function to read data...\n",
    "    def read_data(self, file_name):\n",
    "        try:\n",
    "            # get the filename...\n",
    "            file_name = self.file_path + file_name\n",
    "            file = open(file_name,'r',encoding='cp1252')\n",
    "            filedata = file.readlines()\n",
    "\n",
    "            # create a complete article...\n",
    "            complete_article = ' '.join(filedata)\n",
    "            complete_article = complete_article.replace('\\n','').strip().lower()\n",
    "            \n",
    "            return complete_article\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(traceback.format_exc(e))\n",
    "        \n",
    "    # function to tokenize the data...\n",
    "    def tokenize_data(self, news_data):\n",
    "        # Tokenize the articles...\n",
    "        tokenized_article = sent_tokenize(news_data)\n",
    "        return tokenized_article\n",
    "    \n",
    "    # function to build similarity matrix...\n",
    "    def generate_similarity_score(self, sentence1, sentence2):\n",
    "        try:\n",
    "            # create the vectors....\n",
    "            sparse_matrix = self.vectorizer.fit_transform([sentence1, sentence2])\n",
    "\n",
    "            # get the feature names...\n",
    "            feature_names = self.vectorizer.get_feature_names()\n",
    "\n",
    "            dense = sparse_matrix.todense()\n",
    "\n",
    "            denseList = dense.tolist()\n",
    "\n",
    "            # create a dataframe\n",
    "            df = pd.DataFrame(denseList, columns=feature_names)\n",
    "\n",
    "            vector1 = list(df.iloc[0])\n",
    "            vector2 = list(df.iloc[0])\n",
    "\n",
    "            # build cosine similarity score...\n",
    "            cos_distance = cosine_distance(vector1, vector2)\n",
    "\n",
    "            cosine_similarity = (1 - cos_distance)\n",
    "\n",
    "            return cosine_similarity\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(traceback.format_exc(e))\n",
    "\n",
    "    # function to create a similarity matrix...\n",
    "    def generate_similarity_matrix(self, tokenized_article):\n",
    "        try:\n",
    "            # Create a similarity matrix...\n",
    "            similarity_matrix = np.zeros((len(tokenized_article), len(tokenized_article)))\n",
    "\n",
    "            # Iterate over the sentences...\n",
    "            for index1 in range(0, len(tokenized_article)):\n",
    "                for index2 in range(0, len(tokenized_article)):\n",
    "                    if index1 == index2:\n",
    "                        similarity_matrix[index1][index2] = 1.0\n",
    "                    else:\n",
    "                        # Build a similarity matrix here....\n",
    "                        similarity_matrix[index1][index2] = self.generate_similarity_score(tokenized_article[index1], tokenized_article[index2])\n",
    "\n",
    "            return similarity_matrix\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(traceback.format_exc(e))\n",
    "            \n",
    "    # function to rank the sentences...\n",
    "    def rank_sentences(self, similarity_matrix, tokenized_article, top_n=5):\n",
    "        summarized_text = []\n",
    "        # Create a sentence similarity graph...\n",
    "        sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "        scores = nx.pagerank(sentence_similarity_graph)\n",
    "        ranked_sentence = sorted(((scores[i], s) for i, s in enumerate(tokenized_article)), reverse=True)\n",
    "        \n",
    "        for i in range(top_n):\n",
    "            summarized_text.append(\"\".join(ranked_sentence[i][1]))\n",
    "        \n",
    "        # Step 6 - output the summarize text\n",
    "        summarized_text = \" \".join(summarized_text)\n",
    "        summarized_text = \" \".join(summarized_text.split())\n",
    "        summarized_text = summarized_text.strip()\n",
    "        \n",
    "        return summarized_text \n",
    "        \n",
    "    # function to summarize the text...\n",
    "    def summarize_text(self, file_name):\n",
    "        # Step 1: Read the Articles...\n",
    "        complete_article = self.read_data(file_name)\n",
    "        \n",
    "        # Step 2: Tokenize the data...\n",
    "        tokenized_article = self.tokenize_data(complete_article)\n",
    "        \n",
    "        # Step 3: Generate Similarity Matrix...\n",
    "        similarity_matrix = self.generate_similarity_matrix(tokenized_article)\n",
    "        \n",
    "        # Step 4: Rank the Sentences...\n",
    "        summarized_text = self.rank_sentences(similarity_matrix, tokenized_article)\n",
    "        \n",
    "        return summarized_text\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"News_2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_parse = NewsParsing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_text = obj_parse.summarize_text(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"while we don\\'t know which of these apps have implemented the new sdk, we can confirm that agora has released the sdk and has followed up with its developers to urge them to implement the update,\" povolny told zdnet. mcafee said it discovered this issue last year, in april, during a security audit for temi, a personal robot used in retail stores, which also supports audio and video calling.a subsequent investigation also found clues that this behavior also impacted other apps using the sdk.steve povolny, head of advanced threat research at mcafee, told zdnet in an email last week that they notified agora of their findings and that the company responded by releasing a new sdk in december 2020 that was not vulnerable to cve-2020-25605. any attacker sitting on the same network as a targeted user could intercept the traffic in the initial phases of a call, extract call identifiers, and then join the call without being detected. a small library that provides audio and video calling capabilities contains a bug that can allow attackers to join audio and video calls without being detected.the bug —discovered by security firm mcafee, and tracked as cve-2020-25605— impacts the software development kit (sdk) provided by agora, a us company specialized in providing real-time communication tools.apps that use this sdk for audio and video calling capabilities include the likes of meetme, skout, nimo tv, temi, dr. first backline, hike, bunch, and talkspace.in a report published today, mcafee says it found apps sending call operational data unencrypted over the air. \"neither the agora team nor mcafee has found evidence of the vulnerability being exploited in the wild.\"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
